# -*- coding: utf-8 -*-
"""Bid Optimizers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EwhyIGapMFjPb85Am_0iDIiV0wJy5y3O

# 1. Import Libraries
"""

import pandas as pd
from google.colab import drive
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier

# Mount Google Drive
drive.mount('/content/drive')

# Path to your file in Google Drive
file_path = '/content/drive/My Drive/bid_report.xlsx'

"""# 2. Read Data"""

# Read Excel file, skipping the first row and using the second row as the header
data_frame = pd.read_excel(file_path, engine='openpyxl', header=1)

# Display headers with their indices
print("Headers with Indices:")
for idx, col_name in enumerate(data_frame.columns):
    print(f"{idx}: {col_name}")

"""# 3.Pre-processing"""

# List of column indices to drop
columns_to_drop = [0, 2, 15, 16, 18, 19, 20, 21 ,24, 25, 26, 27, 29, 30,33]

# Drop columns by their index
data_frame = data_frame.drop(data_frame.columns[columns_to_drop], axis=1)

# Display remaining columns
print("Remaining Headers with Indices:")
for idx, col_name in enumerate(data_frame.columns):
    print(f"{idx}: {col_name}")

# List of columns to replace NaN with 0
columns_to_replace = ['region', 'lineitem_id', 'adsize_id', 'bidprice', 'pub_rev', 'winning_domain', 'winning_seat_id']

# Replace NaN values with 0 in the specified columns
data_frame[columns_to_replace] = data_frame[columns_to_replace].fillna(0)

# Display first 10 rows of the DataFrame
print("\nFirst 10 Rows:")
print(data_frame.head(10))

# Check for NaN values in each column
columns_with_nan = data_frame.columns[data_frame.isna().any()].tolist()

# Display the columns with NaN values
if columns_with_nan:
    print("Columns containing NaN values:")
    print(columns_with_nan)
else:
    print("No columns contain NaN values.")

# Add target column: 1 if bidprice > 0, else 0
data_frame['target'] = (data_frame['bidprice'] > 0).astype(int)

# Define categorical and numerical features
categorical_features = ['pub_id', 'ad_unit_id', 'ad_type_id', 'platform_id', 'country', 'make', 'model', 'dsp_id']
numerical_features = ['tmax', 'schain_length']

# Date features: Convert date column to datetime and extract useful features
data_frame['Date'] = pd.to_datetime(data_frame['Date'])
data_frame['day_of_week'] = data_frame['Date'].dt.dayofweek
data_frame['hour_of_day'] = data_frame['Date'].dt.hour

# Ensure all categorical columns are of string type
for col in categorical_features:
    data_frame[col] = data_frame[col].astype(str)

# Include the new date features in numerical features
numerical_features.extend(['day_of_week', 'hour_of_day'])

"""# 4. Split data into training and testing sets"""

X = data_frame[categorical_features + numerical_features]
y = data_frame['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Ensure consistent types in training and test datasets
X_train[categorical_features] = X_train[categorical_features].astype(str)
X_test[categorical_features] = X_test[categorical_features].astype(str)

"""# 5. Build and train the model using Random Forest"""

# Apply scaling to numerical features and OneHotEncoding to categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ]
)

model = RandomForestClassifier(random_state=42)
# Create the pipeline with the preprocessor and classifier
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Step 3: Make predictions on the test data
y_pred = pipeline.predict(X_test)

# Step 4: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')

# Optional: Display confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Optional: Display classification report (precision, recall, F1-score)
class_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(class_report)

"""# 7. Test the model"""

# 5. Evaluate the model
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities
y_pred = model.predict(X_test)  # Get class predictions

# Calculate evaluation metrics
roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"ROC-AUC Score: {roc_auc}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""# 8. Save the model for deployment"""

# 6. Save the model for deployment
import joblib
joblib.dump(model, 'dsp_bid_model.pkl')

